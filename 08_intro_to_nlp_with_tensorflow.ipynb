{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263be215-0b4c-4ae6-b29f-9b647f15d871",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "NLP is focused on deriving information from natural language (text or speech).\n",
    "\n",
    "Another common term for NLP problems is sequence to sequence (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6757e258-3cb5-42bc-9382-12474f5462db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-000cd691-3ea0-4d29-9ce1-2ce917dbe716)\n"
     ]
    }
   ],
   "source": [
    "## Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85ad1bb-15e4-4b24-ab7f-9e948ca497ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 14:55:52.236483: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import functions from helper_functions.py\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c25bb2-cd4e-4e7b-95c5-9ee3bf92ca93",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "I am using the the [Kaggle's Introduction to NLP dataset](https://www.kaggle.com/competitions/nlp-getting-started), a set of tweets for binary classifications of disasters.\n",
    "\n",
    "I will be downloading the dataset from [my course github here](https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c03dc5d-c110-439d-812a-8a28ee51de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-25 14:56:47--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.115.128, 172.253.122.128, 142.251.16.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.115.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: ‘nlp_getting_started.zip.2’\n",
      "\n",
      "100%[======================================>] 607,343     --.-K/s   in 0.01s   \n",
      "\n",
      "2023-01-25 14:56:47 (45.2 MB/s) - ‘nlp_getting_started.zip.2’ saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloads the dataset from google storage\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e7c453-267c-4dcf-8a1e-76fca93cc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae5ed1-b8e6-49c9-88e3-713b39372613",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "Need to visualize our text dataset using Python's Pandas.\n",
    "\n",
    "**NOTE**: A Pandas dataframe can only be the same size as RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4d0576-fb7a-4dff-a363-c2c8ec0f9a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65799bf-2de3-4e8f-a85f-82c4b64c2bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33c2873-c5c8-4be7-9e64-ee2255728a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out test dataframe \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48be250d-f11a-4484-a4be-dec53ec3578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantity of examples of each class\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9ad90-513f-4ca7-8f8d-ac9b48acf3f0",
   "metadata": {},
   "source": [
    "> This is fairly balanced, no balancing required. Otherwise look up imbalanced classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8780db46-ba31-47e5-979f-55ae08c9f9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total samples\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9dd16d9-f041-471f-a055-c782c8d97df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not a real disaster)\n",
      "Text:\n",
      "If you find your patio table umbrella and chairs flipped over and suspect foul play (instead of windstorm) you may be a suspense writer.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not a real disaster)\n",
      "Text:\n",
      "Oh wait I expected to go a totally different route in LS that was derailed by another barely passing grade in a required course. Super.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/RlPTtkBG4W\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Motorcyclist bicyclist injured in Denver collision on Broadway: At least two people were taken to a localÛ_ http://t.co/ozK1QHJVfh\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Oregon's biggest wildfire slows growth http://t.co/P0GoS5URXG via @katunews\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize random training samples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 5) # create random indexes not higher than total samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index: random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not a real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343de767-e15f-4059-a867-6ae1a48f4d3a",
   "metadata": {},
   "source": [
    "### Split data into training and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d088c6-6111-4901-aa47-69cb415ca4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0546244e-aec7-45b7-aeb3-22d613e7b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_test_split to split training data into train and validation splits\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                           train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                           test_size=0.1,\n",
    "                                                                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ee6c3c-402c-4f57-abc0-f5bb6adcd5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the lengths of each split\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10d97e3-2e8a-445f-b2ac-7f9ec88a3190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c57882-e4a5-4f0c-9418-13b492ec9980",
   "metadata": {},
   "source": [
    "## converting text into numbers\n",
    "\n",
    "There are multiple ways to convert text to numbers:\n",
    "* Tokenization - direct mapping of token (word or character) to a number\n",
    "* Embedding - Creates a vector for each word of arbitrary length (128, 256, 512, etc) and creates a matrix to support showing relationships. The embedding matrix acts as a layer that can be trained as more text is analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b8324-8f91-41a4-8514-80abc5f2cf74",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb90fcb-e031-4be1-ae85-a822d17377da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "       'Imagine getting flattened by Kurt Zouma',\n",
       "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the data\n",
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e583b3-8e96-4834-a435-5b5573a1afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in our vocabulary (automatically add <OOV>)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # lowercase and removes punctuation\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None, # create groups of n-words\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None, # pads each sequence to be the same length as the longest sequence is\n",
    "                                    pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1798b8f-1795-410f-88a8-afad8414abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average number of tokens (words) in the training tweets\n",
    "total_words = sum([len(i.split()) for i in train_sentences])\n",
    "\n",
    "# this produces a float so we round to get an int\n",
    "average_words = round(total_words / len(train_sentences))\n",
    "\n",
    "average_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c940807-20f2-41a5-a787-e58be1319a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be, condenses sentences to this max length\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length, # max vocab of 10k words\n",
    "                                   output_mode=\"int\",\n",
    "                                   output_sequence_length=max_length) # truncate or pad to match the maximum sequence length of 15 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1378f456-5838-41e1-acac-f5d241375a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ed1debb-1ec4-4097-a4d8-b0730fd96fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9043e50e-aadf-4625-a954-6a8e169c2a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " quick shut down the show take the stage down evacuate everyone from mthe premises Louis is upset\n",
      "\n",
      " Vectorized Version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[1787, 1476,  134,    2,  431,  167,    2, 1469,  134,  279,  238,\n",
       "          20,    1, 9610, 1193]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original Text:\\n {random_sentence}\\n\\n Vectorized Version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc576a98-2958-483f-906a-41f552964810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# find the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in training data\n",
    "top_5_words = words_in_vocab[:5] # get most common words\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {top_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de28dd8-9ebd-470e-8596-b0b4cb630b11",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding Layer\n",
    "\n",
    "To make the embedding I will be using the TensorFlow Embedding Layer via `tf.keras.layers.Embedding`\n",
    "\n",
    "Params for Embedding Layer:\n",
    "* `input_dim` - Size of the vocabulary\n",
    "* `output_dim` - the size of the output embedding vector, the vector representation of the word will be this many float values long\n",
    "* `input_length` - length of sequences being passed to the embedding layer (set to 15 earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aeb2d80-03ea-4350-b3ca-bb95a6d38bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128,\n",
    "                             input_length=max_length) # how long is each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "466745fd-a8a9-4108-a3d0-9fa262bebe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Emergency responders prepare for chemical disaster through Hazmat training. http://t.co/q9zixCi8E6\n",
      "\n",
      "Embedded Version:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.04044591, -0.02900395,  0.00997239, ...,  0.03864953,\n",
       "         -0.00851812, -0.0404544 ],\n",
       "        [ 0.02330944,  0.04993999,  0.02765414, ..., -0.01541353,\n",
       "         -0.03966758, -0.01031746],\n",
       "        [-0.04903242, -0.0224415 ,  0.04273274, ...,  0.02645764,\n",
       "         -0.03135502, -0.03429221],\n",
       "        ...,\n",
       "        [-0.03267145, -0.03186846,  0.01067818, ..., -0.03734125,\n",
       "          0.03614335, -0.03921503],\n",
       "        [-0.03267145, -0.03186846,  0.01067818, ..., -0.03734125,\n",
       "          0.03614335, -0.03921503],\n",
       "        [-0.03267145, -0.03186846,  0.01067818, ..., -0.03734125,\n",
       "          0.03614335, -0.03921503]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original Text:\\n {random_sentence}\\n\\nEmbedded Version:\\n\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321a4a1-0214-4e12-9473-f4bb18dbf715",
   "metadata": {},
   "source": [
    "> 3 dimensions: 1 sequence, 15 tokens, 128 float values representing each token (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "294f4f8e-9b9c-4b8d-9ac8-807d1c5bed47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 0.04044591, -0.02900395,  0.00997239, -0.00868595, -0.00701711,\n",
       "        -0.02441148,  0.02870316, -0.00010179, -0.02191069, -0.03122919,\n",
       "        -0.00932238, -0.02784938,  0.02494702,  0.03708695,  0.02900923,\n",
       "         0.01676026,  0.04741515,  0.03222844, -0.04998943,  0.00816886,\n",
       "        -0.04940176, -0.03884759,  0.04566295, -0.03069859, -0.02465651,\n",
       "        -0.01160507, -0.04732863,  0.03235627,  0.03983065, -0.03149676,\n",
       "         0.03988333, -0.04337195, -0.01219127,  0.0164099 , -0.03149553,\n",
       "         0.01813369,  0.01193737, -0.00101575,  0.02224777, -0.01937242,\n",
       "        -0.03313792, -0.00638754,  0.03997408,  0.02434206,  0.04397711,\n",
       "         0.00078727, -0.03038983, -0.04513228, -0.02476666, -0.03299035,\n",
       "        -0.03510851,  0.00858132,  0.04969701,  0.01831826,  0.00352351,\n",
       "        -0.00194596, -0.0178342 , -0.00087573, -0.02530595, -0.00017934,\n",
       "         0.03657316,  0.02232499, -0.0454228 , -0.04936112, -0.00162574,\n",
       "         0.02878333, -0.03477608, -0.03569437,  0.02768094,  0.01726805,\n",
       "        -0.01190763, -0.00966426,  0.01525221,  0.04761301,  0.0114108 ,\n",
       "         0.03885952,  0.04947102,  0.04014906,  0.00276468, -0.04500831,\n",
       "         0.04519064, -0.04956681,  0.01579255, -0.00913726,  0.03058365,\n",
       "        -0.01457119,  0.00614394,  0.00278859,  0.01147791, -0.00106872,\n",
       "        -0.04134451,  0.02780903,  0.00145494,  0.02221875,  0.01665293,\n",
       "         0.04596971, -0.04215261, -0.02365391,  0.02565725, -0.04279722,\n",
       "        -0.02378372,  0.04430011, -0.0311182 ,  0.04146537,  0.01750222,\n",
       "         0.02004519,  0.02521705, -0.0410736 , -0.02418879,  0.00172355,\n",
       "        -0.013767  , -0.03759847,  0.04347211, -0.0274124 ,  0.02856088,\n",
       "        -0.00120071,  0.03031767,  0.03548158, -0.00224676, -0.04969161,\n",
       "         0.03967602,  0.03240894,  0.04483462, -0.00847831, -0.00688409,\n",
       "         0.03864953, -0.00851812, -0.0404544 ], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'Emergency responders prepare for chemical disaster through Hazmat training. http://t.co/q9zixCi8E6')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape,  random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54571f63-c920-4ec0-bcae-15f2ae53f302",
   "metadata": {},
   "source": [
    "## modeling a text dataset (series of experiments)\n",
    "\n",
    "The modeling experiements will be:\n",
    "\n",
    "* Model 0: Naive Bays (baseline, sci-kit learn)\n",
    "* Model 1: Feed-forward neural network (dense model)\n",
    "* Model 2: LSTM Model (RNN)\n",
    "* Model 3: GRU Model (RNN)\n",
    "* Model 4: Bidirectional-LSTM model (RNN)\n",
    "* Model 5: 1D Convolutional Neural Network (CNN)\n",
    "* Model 6: TensorFlow Hub Pretrained Feature Extractor (using transfer learning for NLP)\n",
    "* Model 7: Same as model 6 with 10% of the training data (to explore a data constrained scenario)\n",
    "\n",
    "Steps for modeling:\n",
    "1. Create a model\n",
    "2. Build a model\n",
    "3. Fit a model\n",
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b2888-efe6-4779-b39c-3b1e48473b70",
   "metadata": {},
   "source": [
    "### Model 0: Baseline model\n",
    "\n",
    "To build the baseline, I'll use Sklearn's Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers.\n",
    "\n",
    "> It seems commonplace to use non-DL machine learning models to establish a baseline only moving on to Deep Learning when ready to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d75fc9c-41fc-48a3-938d-32b48505fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create tokenization and modeling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf8a78ba-2844-4927-a79b-18d40a0c2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline achieves accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Baseline achieves accuracy of: {baseline_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1677fe6-ba5e-4ea7-8ce6-2b282deb029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5f072-7eae-4996-8273-46939f5420f3",
   "metadata": {},
   "source": [
    "### creating an evaluation function for model experiments\n",
    "\n",
    "Using the following metrics:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd2c5a6-7efe-4fcd-8095-4b995fc15b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to evaluate: accuracy, precision, recall, F1-Score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall, and f1 score for a binary classification model.\n",
    "    \"\"\"\n",
    "    # calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_preds) * 100\n",
    "    # calculate model precision, recall, and f1-score 'weighted' average (supports label imbalance)\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_preds, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\": model_precision,\n",
    "                     \"recall\": model_recall,\n",
    "                     \"f1\": model_f1}\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bbf22fb-1e3f-4adb-9301-54f0fa825e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_preds=baseline_preds)\n",
    "\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc54d6-4aad-459f-b775-c52601299d1a",
   "metadata": {},
   "source": [
    "## model 1: a simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e2cccf8-21d9-4988-96c7-14d73c6e8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensorboard callback (create new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4e8ce59-3659-4973-84e9-08557d237a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model with the funcitonal api\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # one dimensional and of type string only\n",
    "x = text_vectorizer(inputs) # turn input text into numbers\n",
    "x = embedding(x) # create an embedding of the numeric inputs\n",
    "x = layers.GlobalAveragePooling1D(name=\"global_avg_pool_layer\")(x) # condence feature vector for each token to one vector\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, binary outputs = 1 and activation of sigmoid\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cff7614-1522-4fbb-9411-5286b4d0bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glob  (None, 128)              0         \n",
      " alAveragePooling1D)                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beb21453-3a36-4069-aca2-8cd5aed932bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13deda3b-f28c-4c82-8cdd-f0c5bd45cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20230125-145713\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 15s 5ms/step - loss: 0.6088 - accuracy: 0.6885 - val_loss: 0.5317 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.4405 - accuracy: 0.8196 - val_loss: 0.4680 - val_accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3455 - accuracy: 0.8621 - val_loss: 0.4559 - val_accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2831 - accuracy: 0.8934 - val_loss: 0.4662 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2362 - accuracy: 0.9142 - val_loss: 0.4833 - val_accuracy: 0.7848\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model_1_history = model_1.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                    experiment_name=\"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3ef3593-cb8f-453a-8879-6790ebed9321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4832990765571594, 0.7847769260406494]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbae942a-8714-4975-9af0-c6be6449de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95fa2a85-b739-4186-882b-098d9bc903ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3363727 ],\n",
       "       [0.8122002 ],\n",
       "       [0.9970092 ],\n",
       "       [0.12435926],\n",
       "       [0.09801292]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the probability matrix\n",
    "model_1_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdbbfc5f-cf12-469d-a531-0cd3d1a7f155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model prediction probabilities to model format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d75e596-ea94-4643-87ea-ea2534ea5713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.4776902887139,\n",
       " 'precision': 0.7904782347685264,\n",
       " 'recall': 0.7847769028871391,\n",
       " 'f1': 0.7814477888250547}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model_1 results\n",
    "model_1_results = calculate_results(val_labels,\n",
    "                                    model_1_preds)\n",
    "\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ec1e617-bc7f-4321-9cd6-10faa1a8b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check all values of model_1 against baseline\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05b022-f31d-4f89-95fd-6aecde0ff8de",
   "metadata": {},
   "source": [
    "> We see that only the F1 score resulted in a higher value over the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c51422-3e00-4401-8fe7-66cc5145ad5b",
   "metadata": {},
   "source": [
    "## Visualizing learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c41e965-68e2-437a-bd75-c10de91b1c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a179b282-a63f-47f4-8498-6056d93f08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glob  (None, 128)              0         \n",
      " alAveragePooling1D)                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f2bf443-1249-44f5-b65a-5a5644233d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# get the weight matrix of embedding layer (numerical representation of each token in our training data)\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab and embedding_dim (output dim of our embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975db12a-5dcc-4e05-a952-415d7d78c070",
   "metadata": {},
   "source": [
    "Tensorflow has a tool called projector: http://projector.tensorflow.org/\n",
    "\n",
    "Tensorflow embeddings documentation: https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6b7a75f-dc81-446c-8474-683fe6fe0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding files, this code is in embedding documentation\n",
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cc445-eb56-48ff-9084-417eb01fb1c4",
   "metadata": {},
   "source": [
    "> Manually downloaded the `metadata.tsv` and `vectors.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54dc5c9-564e-47a0-adc1-61311a64582e",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "RNN's are primarily used for sequence data.\n",
    "\n",
    "RNN's use the representation of a previous input for a later representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55fcbe-cce9-413b-84d2-6542381ee7b5",
   "metadata": {},
   "source": [
    "## Model 2: LSTM\n",
    "\n",
    "LSTM = Long Short-Term Memory\n",
    "\n",
    "Architecture:\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers (RNN/Dense) -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d585105d-4709-4ba4-92a2-1d324ccf972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This kernel is representing the shape incorrectly for LSTM and GRU, this modifies the expected shape of the data\n",
    "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# create an LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.LSTM(units=64, return_sequences=True)(x) # when stacking RNN cells together, need to return_sequences\n",
    "x = layers.LSTM(64)(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "133b1c11-ceaf-4241-91da-18e7dc73e34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get a summary of model_2\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bea29cb7-4c11-4d3a-a7f8-cafe60f2a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d6d50b4-1314-4c22-b717-5ccfc9ab737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20230125-145734\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 31s 8ms/step - loss: 0.2196 - accuracy: 0.9178 - val_loss: 0.5329 - val_accuracy: 0.7822\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1559 - accuracy: 0.9413 - val_loss: 0.5833 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1275 - accuracy: 0.9505 - val_loss: 0.6541 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1064 - accuracy: 0.9606 - val_loss: 0.7135 - val_accuracy: 0.7795\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0908 - accuracy: 0.9651 - val_loss: 0.8451 - val_accuracy: 0.7717\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"model_2_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c21c24a4-0553-4b3b-8df8-4ad2fba64604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8630134e-02],\n",
       "       [8.9566916e-01],\n",
       "       [9.9969900e-01],\n",
       "       [1.0713756e-01],\n",
       "       [4.7146552e-04],\n",
       "       [9.9744833e-01],\n",
       "       [6.3294834e-01],\n",
       "       [9.9974746e-01],\n",
       "       [9.9957126e-01],\n",
       "       [4.6334100e-01]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions with LSTM model\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61618a2b-851c-4c59-96f3-db1402b3f2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model 2 pred probs to labels\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b575af2-ae20-4401-b6e8-6c17144a1616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.16535433070865,\n",
       " 'precision': 0.7738402637184817,\n",
       " 'recall': 0.7716535433070866,\n",
       " 'f1': 0.7693184984034235}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model 2 results\n",
    "model_2_results = calculate_results(y_true=val_labels, y_preds=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d6aa9f0-43d3-451d-b985-5f8f7dc40293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "869541e5-c982-42be-a10d-1c10cdaf3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GRU model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(1, activation=\"tanh\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa4a23da-d48f-4b5b-abb3-c93b97e18f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get model 3 summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c45a116-9e33-4c2a-8d5c-bda7e8dac288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60f57d65-319c-41a0-a5ac-7711277a9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20230125-145811\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 8ms/step - loss: 0.3326 - accuracy: 0.9145 - val_loss: 1.5927 - val_accuracy: 0.7782\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1695 - accuracy: 0.9585 - val_loss: 1.2300 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1509 - accuracy: 0.9698 - val_loss: 1.7005 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1406 - accuracy: 0.9739 - val_loss: 1.7896 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1539 - accuracy: 0.9707 - val_loss: 1.5222 - val_accuracy: 0.7651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64e1f952d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model 3\n",
    "model_3.fit(train_sentences,\n",
    "            train_labels,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            epochs=5,\n",
    "            callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                   \"model_2_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2baa6ce-44ac-4613-964a-10f1c115a709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions with GRU model\n",
    "model_3_pred_probs = model_3.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e4ebdb6-ddb6-48df-9010-f1c97cef9975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model 3 pred probs to labels\n",
    "model_3_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b16c3cef-c63f-482a-8ab2-95d95f75fc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.16535433070865,\n",
       " 'precision': 0.7738402637184817,\n",
       " 'recall': 0.7716535433070866,\n",
       " 'f1': 0.7693184984034235}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model 3 results\n",
    "model_3_results = calculate_results(y_true=val_labels, y_preds=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2744029-7ce3-48d8-9529-24fe9065955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b3a18-76fa-4e4a-9945-799abf9cf3be",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45d3eb00-c177-4829-8d0e-0187564a9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bidirectional RNN\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_bidirectional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60e66455-33d1-4110-aecf-944f42a2ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83cc9a69-4c96-45c5-bb46-59519b6ae8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model_4.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9a7b88f-f099-4937-8887-050e9a464549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_bidirectional/20230125-145821\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 12ms/step - loss: 0.1209 - accuracy: 0.9626 - val_loss: 0.7757 - val_accuracy: 0.7730\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0670 - accuracy: 0.9749 - val_loss: 1.1141 - val_accuracy: 0.7677\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0539 - accuracy: 0.9772 - val_loss: 0.9859 - val_accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0445 - accuracy: 0.9794 - val_loss: 1.4316 - val_accuracy: 0.7769\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0412 - accuracy: 0.9815 - val_loss: 1.3530 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "# fit data\n",
    "model_4_history = model_4.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"model_4_bidirectional\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "959bab14-91d1-49c7-a053-664deed71b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_4_pred_probs = model_4.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d630f067-80b5-4a37-9665-a5988dc811a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pred probs to pre labels\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "116fbbcb-794b-4888-a78a-fa8723b18d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.37795275590551,\n",
       " 'precision': 0.7635318416248198,\n",
       " 'recall': 0.7637795275590551,\n",
       " 'f1': 0.7628883537445144}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the results of model_4\n",
    "model_4_results = calculate_results(y_true=val_labels,\n",
    "                                    y_preds=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074aa466-c192-425c-9304-ab0db765fdc8",
   "metadata": {},
   "source": [
    "## CNN for text and other types of sequences\n",
    "\n",
    "CNN's are typically for images, images are 2D. Text is 1D, `Conv1D` is what I'll be using\n",
    "\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layer(s) (typically Conv1D + pooling) -> Outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69155c-acd8-42db-a6d2-73fe288ce0c1",
   "metadata": {},
   "source": [
    "### Model 5: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0a9a34a-cbcb-4bd5-bdd0-787056712e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out embedding layer, Conv1D layer and max pooling layer\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sequence into embedding\n",
    "conv_1d = layers.Conv1D(filters=32,\n",
    "                        kernel_size=5,\n",
    "                        activation=\"relu\",\n",
    "                        padding=\"valid\")\n",
    "\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # get the most important feature / get feature with the highest value\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3010a042-56f4-4eb0-8f2a-566b3e5179c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build conv1d model\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", padding=\"valid\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# compile\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# summary of model\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6709872c-5f79-42b4-88c5-04f2548ca289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20230125-145902\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 6ms/step - loss: 0.1204 - accuracy: 0.9637 - val_loss: 0.8890 - val_accuracy: 0.7743\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0743 - accuracy: 0.9733 - val_loss: 1.0518 - val_accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0610 - accuracy: 0.9777 - val_loss: 1.0920 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0570 - accuracy: 0.9759 - val_loss: 1.1485 - val_accuracy: 0.7572\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0523 - accuracy: 0.9788 - val_loss: 1.1715 - val_accuracy: 0.7612\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"Conv1D\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71badc6e-1a45-4a91-8025-1910833e353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.0851688e-01],\n",
       "       [8.3023393e-01],\n",
       "       [9.9991596e-01],\n",
       "       [7.1045786e-02],\n",
       "       [4.4722270e-07],\n",
       "       [9.9278581e-01],\n",
       "       [9.3197459e-01],\n",
       "       [9.9998200e-01],\n",
       "       [9.9999857e-01],\n",
       "       [7.9919451e-01]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make some predictions\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b02bf169-f23f-4d12-ac6f-95fff1c09b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model 5 pred probs to labels\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d4ee3f2-9b0e-491a-a702-f60c5fc107ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.11548556430446,\n",
       " 'precision': 0.761395918264994,\n",
       " 'recall': 0.7611548556430446,\n",
       " 'f1': 0.7597317731418467}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model 5\n",
    "model_5_results = calculate_results(y_true=val_labels,\n",
    "                                    y_preds=model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74e0a4c8-79fb-43af-8843-6e60c13548b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfe961-dd94-4315-8223-4627f51445b5",
   "metadata": {},
   "source": [
    "## Model 6: TensorFlow Hub Pretrained Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbee26-bd59-4894-b3c4-48d2aacdaa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::flake8==4.0.1=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::jupyter_client==7.3.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nltk==3.6.7=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::astropy==5.1=py310hde88566_0\n",
      "  - conda-forge/noarch::flask==2.2.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbformat==5.7.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::flask-cors==3.0.10=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclient==0.7.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-server==1.3.3=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::shap==0.41.0=py310h769672d_0\n",
      "  - conda-forge/noarch::distributed==2022.10.0=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::ipykernel==6.16.2=pyh210e3f2_0\n",
      "  - conda-forge/noarch::nbconvert-core==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-black==1.1.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::requests==2.28.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::cookiecutter==2.1.1=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::dask==2022.10.0=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::ipywidgets==8.0.2=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::jupyter_console==6.4.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert-pandoc==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtconsole-base==5.2.2=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::requests-kerberos==0.12.0=py310hff52083_3\n",
      "  - conda-forge/noarch::s3transfer==0.6.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==5.1.1=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::spyder-kernels==2.2.1=py310hff52083_1\n",
      "  - conda-forge/linux-64::awscli==1.25.97=py310hff52083_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.16.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook-shim==0.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::numpydoc==1.5.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtconsole==5.2.2=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::sphinxcontrib-websupport==1.2.4=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::nbclassic==0.4.5=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::spyder==5.2.2=py310hff52083_3\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py310hff52083_7\n",
      "  - conda-forge/noarch::notebook==6.4.12=pyha770c72_0\n",
      "  - conda-forge/noarch::hdijupyterutils==0.20.0=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::autovizwidget==0.20.0=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::sparkmagic==0.20.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab==3.3.4=pyhd8ed1ab_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::flake8==4.0.1=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::jupyter_client==7.3.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nltk==3.6.7=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::astropy==5.1=py310hde88566_0\n",
      "  - conda-forge/noarch::flask==2.2.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbformat==5.7.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::flask-cors==3.0.10=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclient==0.7.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-server==1.3.3=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::shap==0.41.0=py310h769672d_0\n",
      "  - conda-forge/noarch::distributed==2022.10.0=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::ipykernel==6.16.2=pyh210e3f2_0\n",
      "  - conda-forge/noarch::nbconvert-core==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-black==1.1.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::requests==2.28.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::cookiecutter==2.1.1=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::dask==2022.10.0=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::ipywidgets==8.0.2=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::jupyter_console==6.4.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert-pandoc==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtconsole-base==5.2.2=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::requests-kerberos==0.12.0=py310hff52083_3\n",
      "  - conda-forge/noarch::s3transfer==0.6.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==5.1.1=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::spyder-kernels==2.2.1=py310hff52083_1\n",
      "  - conda-forge/linux-64::awscli==1.25.97=py310hff52083_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.16.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert==7.2.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook-shim==0.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::numpydoc==1.5.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtconsole==5.2.2=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::sphinxcontrib-websupport==1.2.4=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::nbclassic==0.4.5=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::spyder==5.2.2=py310hff52083_3\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py310hff52083_7\n",
      "  - conda-forge/noarch::notebook==6.4.12=pyha770c72_0\n",
      "  - conda-forge/noarch::hdijupyterutils==0.20.0=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::autovizwidget==0.20.0=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::sparkmagic==0.20.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab==3.3.4=pyhd8ed1ab_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: / "
     ]
    }
   ],
   "source": [
    "%conda install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41038ea5-719e-4e06-b5b7-7ac5299face8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8834/1241465740.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embed_samples = embed([sample_sentence,\n\u001b[1;32m      5\u001b[0m                       \"When you call universeal sentence encoder on a sentence, it converts it to a number\"])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call universal sentence encoder on a sentence, it converts it to a number\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d6255-a6f6-4967-b759-758637921148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
